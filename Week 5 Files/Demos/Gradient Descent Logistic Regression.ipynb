{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c616f212-33f5-4d87-b85b-8a8830779aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df24d55-5cab-4ceb-b21a-60cd9c6608cf",
   "metadata": {},
   "source": [
    "## Here are the functions we need for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db68b7e8-a025-4003-a283-f0bc7dfd56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lambda x, w, b: np.matmul(w,x.T) + b\n",
    "sigmoid = lambda yhat: 1/(1+np.exp(-yhat))\n",
    "loss = lambda y, sigmoid: -(y*np.log(sigmoid)+(1-y)*np.log(1-sigmoid)).mean()\n",
    "dldw = lambda x, y, sigmoid: (np.reshape(sigmoid-y,(50,1))*x).mean(axis = 0)\n",
    "dldb = lambda y, sig: (sig-y).mean(axis = 0)\n",
    "update = lambda a, g, lr: a-(g*lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba4c7a7-70fd-4732-bd20-1b5c3f6c018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b and w are the initial weights all set to 0\n",
    "def GradDescent(X, y, n_iter, alpha, b = 0, w = None):\n",
    "    if(w == None):\n",
    "        w = np.zeros(X.shape[1])\n",
    "    learning_rate = alpha\n",
    "    for i in range(n_iter):\n",
    "        yhat = predict(X,w,b)\n",
    "        sig = sigmoid(yhat)\n",
    "        grad_w = dldw(X,y,sig)\n",
    "        grad_b = dldb(y,sig)\n",
    "        w = update(w,grad_w,learning_rate)\n",
    "        b = update(b,grad_b,learning_rate)\n",
    "    return b,w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10068e62-faac-4191-950a-6cee79284d90",
   "metadata": {},
   "source": [
    "To demonstrate this, let's make some random data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ad01d2-8a33-436b-8bfa-f00499836151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c948fa7-5d79-4969-a125-528a6a1a64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasData = make_classification(n_features = 5, n_samples = 50,random_state = 4)\n",
    "\n",
    "x=clasData[0]\n",
    "y=clasData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3cf6147-309a-4956-8ada-775e5905761b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8bebc-f15a-4ea5-a4ca-de9d71c1545d",
   "metadata": {},
   "source": [
    "This gives us dummy classification data with 5 features and 50 in our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40235201-e455-40ec-a057-b29e4618e97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.728933077054689,\n",
       " array([-0.12818396, -0.28864804, -1.11075534,  0.14203945,  2.54489187]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradDescent(x, y, 2000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361be930-60eb-4706-8a45-f6920e79b069",
   "metadata": {},
   "source": [
    "Compare with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa4aaf8e-33f2-4a1d-887c-c1f550ab9be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5317b28e-ffe1-4a72-b5b4-4362c533ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46372887] [[-0.11404243 -0.19874298 -0.76255719  0.13358927  1.77353138]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x,y)\n",
    "print(model.intercept_,model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682fcaa5-f915-4f2b-b8a9-f8e6d324a08a",
   "metadata": {},
   "source": [
    "It's not the same :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e459d4-ad49-45bb-9f29-cdc916e83ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdual\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Logistic Regression (aka logit, MaxEnt) classifier.\n",
       "\n",
       "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
       "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
       "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
       "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
       "'sag', 'saga' and 'newton-cg' solvers.)\n",
       "\n",
       "This class implements regularized logistic regression using the\n",
       "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
       "that regularization is applied by default**. It can handle both dense\n",
       "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
       "floats for optimal performance; any other input format will be converted\n",
       "(and copied).\n",
       "\n",
       "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
       "with primal formulation, or no regularization. The 'liblinear' solver\n",
       "supports both L1 and L2 regularization, with a dual formulation only for\n",
       "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
       "'saga' solver.\n",
       "\n",
       "Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
       "    Specify the norm of the penalty:\n",
       "\n",
       "    - `None`: no penalty is added;\n",
       "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
       "    - `'l1'`: add a L1 penalty term;\n",
       "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
       "\n",
       "    .. warning::\n",
       "       Some penalties may not work with some solvers. See the parameter\n",
       "       `solver` below, to know the compatibility between the penalty and\n",
       "       solver.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
       "\n",
       "    .. deprecated:: 1.2\n",
       "       The 'none' option was deprecated in version 1.2, and will be removed\n",
       "       in 1.4. Use `None` instead.\n",
       "\n",
       "dual : bool, default=False\n",
       "    Dual or primal formulation. Dual formulation is only implemented for\n",
       "    l2 penalty with liblinear solver. Prefer dual=False when\n",
       "    n_samples > n_features.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "C : float, default=1.0\n",
       "    Inverse of regularization strength; must be a positive float.\n",
       "    Like in support vector machines, smaller values specify stronger\n",
       "    regularization.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    Useful only when the solver 'liblinear' is used\n",
       "    and self.fit_intercept is set to True. In this case, x becomes\n",
       "    [x, self.intercept_scaling],\n",
       "    i.e. a \"synthetic\" feature with constant value equal to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *class_weight='balanced'*\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
       "    data. See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
       "\n",
       "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
       "    To choose a solver, you might want to consider the following aspects:\n",
       "\n",
       "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
       "          and 'saga' are faster for large ones;\n",
       "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
       "          'lbfgs' handle multinomial loss;\n",
       "        - 'liblinear' is limited to one-versus-rest schemes.\n",
       "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
       "          especially with one-hot encoded categorical features with rare\n",
       "          categories. Note that it is limited to binary classification and the\n",
       "          one-versus-rest reduction for multiclass classification. Be aware that\n",
       "          the memory usage of this solver has a quadratic dependency on\n",
       "          `n_features` because it explicitly computes the Hessian matrix.\n",
       "\n",
       "    .. warning::\n",
       "       The choice of the algorithm depends on the penalty chosen.\n",
       "       Supported penalties by solver:\n",
       "\n",
       "       - 'lbfgs'           -   ['l2', None]\n",
       "       - 'liblinear'       -   ['l1', 'l2']\n",
       "       - 'newton-cg'       -   ['l2', None]\n",
       "       - 'newton-cholesky' -   ['l2', None]\n",
       "       - 'sag'             -   ['l2', None]\n",
       "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
       "\n",
       "    .. note::\n",
       "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
       "       with approximately the same scale. You can preprocess the data with\n",
       "       a scaler from :mod:`sklearn.preprocessing`.\n",
       "\n",
       "    .. seealso::\n",
       "       Refer to the User Guide for more information regarding\n",
       "       :class:`LogisticRegression` and more specifically the\n",
       "       :ref:`Table <Logistic_regression>`\n",
       "       summarizing solver/penalty supports.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "    .. versionchanged:: 0.22\n",
       "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
       "    .. versionadded:: 1.2\n",
       "       newton-cholesky solver.\n",
       "\n",
       "max_iter : int, default=100\n",
       "    Maximum number of iterations taken for the solvers to converge.\n",
       "\n",
       "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
       "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
       "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
       "    across the entire probability distribution, *even when the data is\n",
       "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
       "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
       "    and otherwise selects 'multinomial'.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "    .. versionchanged:: 0.22\n",
       "        Default changed from 'ovr' to 'auto' in 0.22.\n",
       "\n",
       "verbose : int, default=0\n",
       "    For the liblinear and lbfgs solvers set verbose to any positive\n",
       "    number for verbosity.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of CPU cores used when parallelizing over classes if\n",
       "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
       "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
       "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors.\n",
       "    See :term:`Glossary <n_jobs>` for more details.\n",
       "\n",
       "l1_ratio : float, default=None\n",
       "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
       "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
       "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
       "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "\n",
       "classes_ : ndarray of shape (n_classes, )\n",
       "    A list of class labels known to the classifier.\n",
       "\n",
       "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "\n",
       "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
       "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
       "\n",
       "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
       "    Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "    `intercept_` is of shape (1,) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
       "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
       "    outcome 0 (False).\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
       "    Actual number of iterations for all classes. If binary or multinomial,\n",
       "    it returns only 1 element. For liblinear solver, only the maximum\n",
       "    number of iteration across all classes is given.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "\n",
       "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
       "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SGDClassifier : Incrementally trained logistic regression (when given\n",
       "    the parameter ``loss=\"log_loss\"``).\n",
       "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The underlying C implementation uses a random number generator to\n",
       "select features when fitting the model. It is thus not uncommon,\n",
       "to have slightly different results for the same input data. If\n",
       "that happens, try with a smaller tol parameter.\n",
       "\n",
       "Predict output may not match that of standalone liblinear in certain\n",
       "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "in the narrative documentation.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
       "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
       "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
       "\n",
       "LIBLINEAR -- A Library for Large Linear Classification\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
       "\n",
       "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
       "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
       "    https://hal.inria.fr/hal-00860051/document\n",
       "\n",
       "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
       "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
       "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
       "\n",
       "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
       "    methods for logistic regression and maximum entropy models.\n",
       "    Machine Learning 85(1-2):41-75.\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.linear_model import LogisticRegression\n",
       ">>> X, y = load_iris(return_X_y=True)\n",
       ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
       ">>> clf.predict(X[:2, :])\n",
       "array([0, 0])\n",
       ">>> clf.predict_proba(X[:2, :])\n",
       "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
       "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
       ">>> clf.score(X, y)\n",
       "0.97...\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\dhurley\\appdata\\local\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     LogisticRegressionCV"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e641a-f713-4a9e-8644-cf8c61ab5c7f",
   "metadata": {},
   "source": [
    "Oh LogisticRegression applies l2 regularisation by default, let's see what happens if we remove that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a16e502-5649-4343-8d57-3f0c7be632f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.72893426] [[-0.1281836  -0.28864813 -1.1107556   0.14204109  2.54489363]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty=None)\n",
    "model.fit(x,y)\n",
    "print(model.intercept_,model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924cbf54-6ed6-4398-928e-dd23a9938262",
   "metadata": {},
   "source": [
    "That's a lot closer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca128e-5800-40f0-92a2-9a1b057cd3b7",
   "metadata": {},
   "source": [
    "## Let's do regularisation ourselves\n",
    "\n",
    "loss and dldw are the things we need to change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26c84488-7de1-4486-bbe7-d191572ce2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossreg = lambda y, sigmoid, l, w: -(y*np.log(sigmoid)+(1-y)*np.log(1-sigmoid)).mean()+ l/(2*len(y))*(w**2).sum()\n",
    "dldwreg = lambda x, y, sigmoid, l, w: (np.reshape(sigmoid-y,(50,1))*x).mean(axis = 0) + (l/len(y))*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "350c8861-18ec-44b0-ab25-46fc2769faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradDescentwithReg(x, y, n_iter, alpha, b = 0, w = None, l = 1):\n",
    "    if(w == None):\n",
    "        w = np.zeros(x.shape[1])\n",
    "    learning_rate = alpha\n",
    "    for i in range(n_iter):\n",
    "        yhat = predict(x,w,b)\n",
    "        sig = sigmoid(yhat)\n",
    "        grad_w = dldwreg(x,y,sig, l, w)\n",
    "        grad_b = dldb(y,sig)\n",
    "        w = update(w,grad_w,learning_rate)\n",
    "        b = update(b,grad_b,learning_rate)\n",
    "    return b,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75c7d28a-f3c3-4132-9b14-2a49f3f08bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.46372835335165286,\n",
       " array([-0.11404314, -0.19874293, -0.76255704,  0.1335889 ,  1.77353078]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradDescentwithReg(x, y, 1000, 1, l=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81914139-a691-482d-a39b-54dc71b8a339",
   "metadata": {},
   "source": [
    "And that's pretty close to the regularised form from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e1b8d-d1e6-4397-b9d3-84d38c6a6d88",
   "metadata": {},
   "source": [
    "## Different amount of regularisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df48888e-7f01-4346-9534-ba7a86b1144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6804317671624744,\n",
       " array([-0.12797542, -0.2718795 , -1.04574135,  0.14158971,  2.40169268]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradDescentwithReg(x, y, 1000, 1, l=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a830ae65-c281-4333-b5ef-56ece54aa7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.14111989]),\n",
       " array([[-0.06336817, -0.08722793, -0.33349453,  0.0777234 ,  0.78975915]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C = 0.1)\n",
    "model.fit(x,y)\n",
    "model.intercept_,model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e02848-6d3f-4f3f-8176-2a70abfdd588",
   "metadata": {},
   "source": [
    "Agh it's different again, why sklearn?\n",
    "\n",
    "# RTM\n",
    "\n",
    "C is not the same as $\\lambda$, C is actually $\\frac{1}{\\lambda}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0136573-f4ba-4c8c-b18a-cafb7166f4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.68043263]),\n",
       " array([[-0.12797468, -0.27187959, -1.04574172,  0.14158944,  2.4016933 ]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C = 1/0.1)\n",
    "model.fit(x,y)\n",
    "model.intercept_,model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5631a-0c0d-4b39-9dc9-0f61a303c40c",
   "metadata": {},
   "source": [
    "Phew we're ok!\n",
    "\n",
    "Always important to read the manual and see what they actually mean about the equivalent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42583137-7d15-4a8e-9539-6c86f772b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradDescentwithRegPlot(x, y, n_iter, alpha, b = 0, w = None, l = 1):\n",
    "    if(w == None):\n",
    "        w = np.zeros(x.shape[1])\n",
    "    learning_rate = alpha\n",
    "    losses = []\n",
    "    for i in range(n_iter):\n",
    "        yhat = predict(x,w,b)\n",
    "        sig = sigmoid(yhat)\n",
    "\n",
    "        logloss = lossreg(y,sig,l, w)\n",
    "        losses.append(logloss)\n",
    "        \n",
    "        grad_w = dldwreg(x,y,sig, l, w)\n",
    "        grad_b = dldb(y,sig)\n",
    "        w = update(w,grad_w,learning_rate)\n",
    "        b = update(b,grad_b,learning_rate)\n",
    "    return losses, b, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd559d94-4286-4a16-a40c-ee9d5f5ad5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, _, _ = GradDescentwithRegPlot(x, y, 1000, 0.1, l=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ef23554-ebfd-4984-8bd9-f33b3512b6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cf5a298290>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a0320-8819-4ed1-81dd-c6e31543b3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
